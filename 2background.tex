\chapter{研究背景}\label{background}
本章主要说明相关研究背景，首先介绍智能体的基本概念；然后介绍当前热门的智能体架构-BDI（Belief-Desire-Intention）智能体架构；再对BDI智能体在社会仿真场景下的相关现有研究进行细致阐述，其中包括对传统意图进展问题的研究，norm约束下BDI智能体决策问题的研究，对维持型目标的研究以及时序逻辑在BDI智能体中的应用；最后介绍一种表示BDI智能体意图的数据结构-目标计划树（Goal-Plan Tree，GPT），并基于此对意图进展问题进行规范定义。
\section{智能体系统}
智能体在现代社会有着广泛的应用，然而不同领域对智能体的定义也不尽相同。本文遵循\cite{DBLP:journals/ker/WooldridgeJ95}中对智能体的定义，智能体是一种处于一定环境下能够自主、智能地完成其他个体指派任务的计算机系统，其遵循感知--执行动作的周期性运作流程。其最大的特点是自主性、社会性、反应性以及主动性。
\begin{itemize}
  \item 自主性：自主性是指智能体能够无需他人详细地编程或指示的情况下自主地进行一系列行为，比如判断、计算和做出动作，这些都取决于智能体自身而非其他个体。这种自主的表现都是基于内部状态和智能体的行为能力，内部状态是智能体对自身所处状态的理解，即智能体认为它是怎么样的、处于什么状态、目的是什么。而行为能力是指智能体根据内部状态进行自主的判断，能做出一些具体的行为来影响其周围的环境或改变自身的处境。自主性是智能体最核心的特性，正是因为这种自主性，当我们委派任务给智能体时或智能体执行时，我们不需要对智能体进行细节的控制，而是让智能体自主地去完成。
  \item 社会性：在多智能体系统（Multi-Agent Systems，MAS）中，智能体之间会进行交互，比如合作完成任务，或者在比赛中竞争，表现出社会性。在社会性的交互中，信息交流是不可或缺的，智能体须通过交流的语言、协议、信息本身对信息进行发送或接收处理，同时智能体根据交互的信息对系统中其它智能体建立认知模型，以更好地使智能体共同完成一些特定目标。
   \item 反应性：智能体能够感知到环境中的事件或发生的一些变化并及时的做出一些决策或行为来进行响应，以便实现指定目标。这种能力使得智能体在完成指定目标的过程中能够更加灵活地应对环境中的变化。
   \item 主动性：智能体能够主动地去尝试并执行完成目标的过程，而不是被动地等待机会或其它个体的指令。
\end{itemize}
% @TODO maybe consider other features?
\section{BDI智能体架构}
实现智能体的方式有多种，不同的实现方式对应了不同的智能体架构。目前已有多种智能体架构被提出，例如慎思型智能体架构（Deliberative agent Architecture），反应型智能体架构（Reactive Agent Architecture）以及混合型智能体架构（Hybird Agent Architecture）。本文的关注重点为BDI智能体架构（Belief-Desire-Intention Agent Architecture）。BDI智能体架构是当前热门且成熟的智能体架构，其起源于Bratman的关于实践推理（Practical Reasoning）的哲学理论\cite{bratman1987intention}。为了对人类的心理状态进行模拟，BDI智能体架构明确地将智能体的内部状态以一组特殊的数据结构表示，即信念、欲望和意图。基于BDI智能体架构的智能体被称为BDI智能体。
% Bielief
在BDI智能体中，信念表示的是BDI智能体所相信的事件或状态，例如在某个位置、目前的速度或当前的外部环境情况等等，然而信念并不一定是事实，仅仅代表BDI智能体的认知，BDI智能体会根据自身对环境的感知或自身做出的行为改变环境进而改变信念，或是将信念作为参考做出一系列的判断或行为。
% Desire
愿望指的是BDI智能体想要实现的环境环境状态。欲望可以是没有具体实现方案去实现的，甚至不可实现的，也可被视为一些未实例化的想法，而并没有确定去如何做，做什么。此外当智能体同时有多个意图时，由于没有实例化或决定实现这些意图，意图与意图之间可以有冲突，例如BDI智能体可以同时拥有进行休眠充电和打扫卫生这两个愿望，即使这二者是相互冲突的。
% 目标
目标（Goal）指的是一组智能体确定要去实现的愿望，也就是说一个目标对应于一个实例化的愿望，即智能体知道要实现什么并且承诺尝试去实现。目标的种类主要有两种：实现型目标（Achievement Goal）和维持型目标（Maintenance Goal）。
% acheivement goal
实现型目标表示智能体想要达到的世界状态，一旦该状态达成，其相应的实现型目标就会被丢弃。例如，智能体有一个去超市电池的实现型目标，一旦购买成功，无需重复购买，智能体便会丢弃该目标。

% Maintenance goal
维持型目标要求智能体在一段时间内（或者一直）维持某一世界状态。和实现型目标不同的是，维持型目标被满足后并不会被丢弃，而是仍然存储于智能体的内部。智能体会对维持型目标持续监控，直到其过期。在此期间一旦维持型目标被破坏，智能体便会尝试修复该目标状态，这种情况下智能体对待维持型目标的方式是被动的，即等到维持型目标被破坏再尝试修复，以被动方式应用的维持型目标被称为被动维持型目标（Reactive Maintenance Goal）。此外，智能体还可以在维持型目标被破坏之前对其是否在将来会被破坏进行预测评估，如果预计在将来的某个时刻维持型目标会被破坏，智能体还可以采取主动措施防止其被破坏，以该种主动方式应用的维持型目标被称为主动维持型目标（Proactive Maintenance Goal）。在本文中，维持型目标是一个重要的研究内容；后文将对维持型目标的相关调度算法进行细致阐述并进行性能评估。

%Intention
意图（Intention）为实例化的计划，用以实现某个目标。而计划的内部--计划体由动作和子目标组成。动作的执行可以直接影响外部环境，而子目标可由其他子计划实现。 当智能体承诺要实现某个目标时，便需要执行一个具体的计划来实现对应的目标。一个目标可能可由多个不同的计划来实现，智能体只要选择其中一个计划执行即可，即智能体会从该目标对应的可选计划中根据具体实际情况，判断并选择一个计划，通过执行选中的计划来完成相应目标，这种多个计划的设计也使得智能体可以灵活应对不同的环境。由于计划体也可能包含子目标，在运行过程中，会递归性地选出并执行多个计划，从具体实现的角度来说，每个意图都是一个栈，里面存储的就是部分实例化的计划，意图的执行也就对应于执行栈中一系列的计划。

% Practical Reasoning
在BDI智能体架构中，智能体进行决策的流程被称为实践推理（Practical Reasoning）。和纯粹的逻辑推理或理论推理不同，实践推理是以行为为导向的，即决定做什么以及如何去做。例如，假设Alice是Bob女儿，Carl是Alice的儿子。则基于逻辑推理可知Carl是Bob的孙子，该种推理方式仅仅依赖智能体的信念以及一些逻辑规则；相较于此，决定该乘坐何种交通工具上学的决策过程则是实践推理的范畴，因为其包含对“如何做”的考虑，是以行为为导向的推理。

% deliberation and means-ends reasoning
Bratman\cite{bratman1987intention}认为实践推理可被看做智能体基于其内部状态，（即信念、愿望等），在多个相互冲突的选择中进行权衡考量，做出合理的决策的过程。
%
具体地，实践推理主要有两步构成：慎思（Deliberation）和手段推理（Means-Ends Reasoning）。
%
在第一步慎思过程中，智能体考虑要去实现哪个目标，即生成目标对应的意图，意味着智能体决定要去尝试实现该目标。
%
而在第二步手段推理过程中，智能体考虑如何实现某个目标，即对于上一步确定要实现的目标，决定执行哪一个计划去实现。
% example
例如，假设智能体现有两个愿望：前往超市购买食物和在家打扫卫生；在经历慎思过程后，智能体决定前往超市。而前往超市有两种途径：走路前往和开车前往；智能体决定走路前往还是开车前往的过程即为手段推理。

\section{社会仿真场景国内外研究现状}
% 考虑分三点：1.传统意图进展问题，2.norm约束下的决策，3.维持型目标下的决策
% SI
在对意图进展问题的研究中，Thangarajah等人\cite{DBLP:journals/jar/ThangarajahP11,DBLP:conf/ijcai/ThangarajahPW03}提出了基于SI （Summary Information）的意图调度算法，其通过自底向上总结信息的方式确定实现目标的必要条件、可能条件、必要结果以及可能结果来推理出意图之间是否存在冲突以及是否存在协同效应。SI算法在智能体程序编译期进行信息总结计算，并在运行期间随着智能体意图的进展动态更新总结信息。此外，SI支持实时判断接收新目标的机制，允许智能体在运行时根据总结信息判断新的目标是否一定会、可能会或一定不会与当前目标相冲突，然后决定是否接收新的目标。然而，该方法假设现有目标的优先级总是大于新目标，若发生冲突则不接收新目标，而不会考虑抛弃现有的冲突目标。

% C
Waters等人\cite{DBLP:conf/atal/WatersPS14,DBLP:conf/aamas/ThangarajahSP12}提出了CB（Coverage Based）意图调度算法，并在后续研究中对其进行了改进\cite{DBLP:journals/aamas/WatersPS15}。CB考虑在编译期对实 现目标计划的健壮性进行数值化刻画，提出在不同环境下智能体实 现某个目标的可执行性（Coverage），以及在同一环境下智能体实现 某个目标的计划选择多样性（Overlap）。在意图进展过程中CB优先考虑实现脆弱的目标，以免因为环境自身的动态性导致目标因无可选计划执行而无法实现。相较于SI，CB重点考虑的是智能体在动态环境下的意图进展。

% MCTS
Yao等人提出了基于蒙特卡洛树搜索（Monte Carlo Tree Search，MCTS）的意图调度算法\cite{yao2017robust,dblp:conf/ijcai/Yao20,DBLP:conf/atal/YaoL16,DBLP:conf/ecai/YaoLT14,DBLP:conf/aaai/YaoLT16,DBLP:conf/ecai/YaoLT16}，其利用MCTS的特点，对决策搜索空间进行启发式搜 索，最终可在任意时间得到决策的最优或次最优解。和SI以及CB不 同，MCTS算法的执行发生在程序运行期，其总是基于当前状态对决 策空间进行搜索，并得出当前决策结果。MCTS调度策略被应用于消 解并行意图间的冲突、开发并行意图交错执行的协同效应、对时间 限制的意图进行调度以及在动态和不确定环境下的意图调度。此外，根据对目前该研究领域的调研，得知基于MCTS的意图调度算法 是当前可验证的最好的意图调度策略。
% 传统意图调度方法的问题
然而，上述方法都没有考虑到社会模拟场景中维持型目标对智能体的影响，缺乏在意图调度是考虑维持型目标的能力。


% maintanance goal
在对维持型目标的研究中，Duff等人提出一种基于SI的意图调度算法\cite{DBLP:conf/atal/DuffHT06}，其在决策过程中加入对维持型目标的考虑：若实现型目标将与维持型目标冲突，执行预防措施（例如补充资源）。 然而，和SI类似，该算法假设当前的目标优先级总是高于新的目标，并且如果新的目标为维持型目标且与当前目标相冲突，维持型目标将被直接抛弃。后来Duff等人在\cite{DBLP:journals/ci/DuffTH14}中拓展了维持型目标的规范语法及语义，但是意图调度策略并没有被改进。Hindriks等人提出了一种处理维持型目标的决策机制\cite{DBLP:conf/dalt/HindriksR07}，该机制直接避免执行破坏维持型目标的行动，保证维持型目标不会被破坏。然而该方法将会导致智能体可执行的行动变少，降低智能体的灵活性。在\cite{DBLP:conf/ecai/ThangarajahHMY14}中，Thangarajah等人分析了维持型目标的进展过程， 提供了一套合理的维持型目标进展模型，但是起没有考虑到意图进展问题。

传统的意图调度算法中也没有考虑到norm对智能体决策的影响。
% norm决策
% BOID
在norm对智能体决策影响的研究领域中，最早或许可以追溯到BOID架构\cite{DBLP:conf/agents/BroersenDHHT01}，BOID架构实质上是对BDI架构的一种延申，赋予其决策时考虑norm的能力。BOID在BDI的基础上引入了智能体性格类型的概念以处理norm与行动、目标之间的冲突。BOID智能体可以是社会型的、无视规范型的等，不同类型的BOID智能体对norm有不同的解释，在解决norm相关冲突时也有不同的策略。例如无视规范型的智能体可能会更倾向于考虑自身目标的实现，而不过多考虑norm对其约束，在norm相关冲突发生时忽略norm的存在。BOID智能体的决策过程是纯粹基于命题规则的。如果在其决策过程中某些规则发生冲突（可能由norm造成），智能体将基于其自身类型进行冲突消解以得到最早确定的决策结果。BOID架构的设计非常简洁，易于理解，但是这种架构并没有广泛的实际应用。很大 一部分原因在于其健壮性低下，仅依靠逻辑推理做出决策，决策过程的时间复杂度高，当规则量庞大时，极其耗时，且不易维护。

% NoA
NoA\cite{DBLP:conf/ijcai/KollingbaumN03}架构基于BDI架构并引入了norm的相关概念，例如义务，禁令以及许可。许可规范了智能体可以做什么（而不一定要去做）。在决策阶段，NoA架构规范了智能体该从计划库中选择哪个计划去执行以遵守norm的相关约束。NoA是完全以norm为主导的架构，它没有自身内部的目标，因此不需要考虑norm与目标的冲突。这使得NoA总是会做出遵守norm的行动。NoA能够处理norm的相关概念，其架构设计也较为简易,易于理解。然而--中仅使用文字对NoA的概括性描述，而缺乏对NoA架构抽象准确的形式化定义。没有内部状态，完全以norm为导向的设计也使得NoA架构适用场景受限，限制了BDI智能体的基本自主性。


NBM（Norm-based Behavior Modification）\cite{DBLP:conf/atal/MeneguzziL09}是对基础BDI架构的一种延伸，其解决的问题主要是智能体在运行时如何去适应新的norm，NBM架构的重点在于提供一种机制使得智能体有能力在运行时去约束某些计划的执行以适应其在运行时所检测到的新的norm。其优点在于其方法依然在BDI的框架下，对于新的norm，创建一个新的计划来处理，将那些norm禁止的计划进行限制，不让其在后续norm生效的情况下执行。该机制可以方便地部署到其它符合BDI架构的智能体系统中。虽然本文实现了在运行时应对新检测的norm的机制，但是没有对norm对于智能体的影响进行精确地刻画，简单认为norm禁止的计划就不能去执行，使得智能体灵活性降低。其研究重点关注的问题也仅限于计划选择，目标选择并不在其考虑范围之内。

% N-2APL
N-2APL\cite{DBLP:conf/aamas/AlechinaDL12}是基于2APL\cite{DBLP:journals/aamas/Dastani08}的智能体编程语言。 N-2APL在2APL的基础上拓展了对norm的表示与处理机制，支持表示义务、禁令等概念，支持在决策过程中处理norm带来的冲突。N-2APL的目标选择与计划选择机制是基于实现目标截止期限以及计划优先级制定的。N-2APL假设计划优先级由编程人员根据norm的影响提前定义。其决策过程主要为： 1.放弃某些低优先级的目标以确保高优先级的目标能够在截至期限内完成；2.选取一个高优先级且能够在截止期限实现的目标；3.根据提前定义好的优先级顺序选择计划执行。
在N-2APL中，有限制条件的义务并没有被考虑，使其应用场景受限。N-2APL通过引入原子计划与非原子计划的概念，以降低可能的执行序列的数量来控制决策算法的时间复杂度并消解计划间并行执行的冲突：原子计划执行时不能被打断，而非原子计划执行过程中可以被其他计划打断。这种设计需要编程人员手动对计划进行分类，不利于大规模实践应用；由于可能执行序列的减少，这种设计也会一定程度上降低了BDI智能体的灵活性。

% N-Jason
N-Jason\cite{DBLP:conf/dalt/LeePLDA14}是一个基于Jason\cite{bordini2007programming}智能体编程语言，支持运行时处理未知norm智能体编程语言。相较于以往的norm处理机制——假设norm在运行前就已经确定，在运行时也不会改变， N-Jason支持运行时对未知norm的识别。如果N-Jason识别到义务norm，则将其转化为一个新的目标待后续阶段实现；如果识别到禁令norm，则将其加入禁令norm集合，供后续进行冲突消解阶段应用。与N-2APL类似，N-Jason 也支持基于截止期限与norm的影响对目标、 计划划分优先级以消解冲突并选择合适的计划执行。然而，不像N-2APL，N-Jason没有考虑多目标之间的选择问题，仅考虑了如何选择计划。作者也提到基于N-Jason产生的的智能体行为可能在编程人员看起来是不可解释、不可预测的，然而却没有提供确切的理论或实践依据。

% v-BDI
v-BDI\cite{DBLP:journals/eaai/MeneguzziROVL15}对BDI架构进行了拓展，在决策时加入了对norm的考虑，其允许智能体在norm冲突发生时，根据自身认识自主地选择是否遵守norm，v-BDI的关键技术在于在计划选择时先对各计划的行动进行模拟运行，然后根据模拟结果对比各个计划的价值，最终选择出价值最大的计划执行。v-BDI依据计划价值来进行计划选择， 当执行计划的收益大于违反norm受到的惩罚时，智能体可以选择去做出违反norm的行为以获得更大的整体收益。这种机制保留了BDI智能体的灵活性与自主性，同时计划选择的标准由模拟结果决定， 不需要编程人员明确给的。编程人员仅需要定义模拟的价值函数即可。v-BDI中对计划价值的计算仅考虑其中的行动，而不考虑子目标，这使得价值计算过程的时间复杂度较低，易于实际的应用。然而v-BDI仅考虑到了计划选择问题，而并没有提及多目标情况下的目标选择问题。仅考虑计划中行动的模拟机制虽然高效，但是必然会因为没有考虑到子目标而导致价值计算不准确。

此外，在非BDI架构领域，也有一些对norm考虑的决策机制被提出。相较于BDI架构，非BDI架构的智能体没有事先由编程人员定义好的计划库，而仅有由一个个独立的行动组成的行动集合。因此，设计非BDI架构智能体norm决策机制最大的挑战在于如何在norm规范下根据一定的规则，基于行动集合生成某些执行序列以供智能体执行，达到设计者的设计目标。

%
\cite{DBLP:conf/atal/ShamsVPV15}使用到了提前设定好的计划，但是其和BDI智能体的差异在于\cite{DBLP:conf/atal/ShamsVPV15}提出的决策过程允许智能体根据norm的影响动态调整执行计划。其在运行时选择执行序列并将其与当前计划相比较，若新执行序列组成的计划的收益大于当前计划，则将新生成计划作为当前计划执行。Norm对智能体行为的约束体现在若智能体做出违反norm的行为将受到一定数值的惩罚。各行动受惩罚值的大小决定了智能体会如何执行计划：智能体总是会执行整体收益（实现目标的收益+违反norm的惩罚）最大的计划。该方法能够通过搜索得到最优的计 划，但是其时间复杂度较高，若可选计划数量较多，则实际应用较为困难；同时其也没有考虑到多个并行执行的计划之间的相互冲突与影响。

% RL
\cite{DBLP:conf/atal/LiMFL15}提出了使用强化学习方法对变化的或位置的norm进行学习，并根据学习结果对智能体自身的行为进行动态调整，以更好地在norm存在的环境中实现目标。\cite{DBLP:conf/atal/LiMFL15f}分别对两种强化学习方法 QLearning 和 SARSA\cite{Sutton2005ReinforcementLA}进行了测试，验证了其有效性并对比分析了 两种方法的特点与适用场景：Q-Learning更加激进，对norm的学习的速度很快，但是不太稳定；SARSA比较保守，对 norm的学习速度慢，但是稳定性高。与—类似，\cite{DBLP:conf/atal/LiMFL15}没有考虑多目标之间的冲突问题，并且当问题规模较大时，这两种基础强化学习方法需要消耗大量计算时间，不利于实际应用。

% argumentation based
\cite{DBLP:conf/ijcai/ShamsVOP16,DBLP:journals/taas/ShamsVOP20,DBLP:conf/atal/ShamsVPV15}提出的方法在不同场景设定下对 norm 的考虑主要体 现在可解释性，其实用 Argumentation Dialog 的方法试图对智能体 在 norm 约束下的行为进行解释，以提高人类对智能体程序的信任 度。同时，其考虑到了 norm 与智能体目标之间的冲突问题，提供了 一套消解冲突的机制。然而，与\cite{DBLP:conf/atal/LiMFL15}类似，在各个场景下该机制的 时间复杂度较高，并且没有考虑多个目标之间的冲突。

目前在BDI智能体研究领域中，已有多种对智能体的功能性进行拓展的方法被提出以提升其适用性。鉴于大多数对智能体目标的研究 都着重关注实现型目标和维持型目标，Dastani等人\cite{DBLP:conf/atal/DastaniRW11}提出以时序逻辑来表示目标，以拓展智能体支持的目标种类。在他们提出的框架中，目标的表示不仅包含基本命题，还包含时序连接符号如$\diamond$（最终），$\square$（总是）以及$U$（直到）。在具体应用时，由于大多数智能体语言和平台都不支持时序逻辑，需要将时序逻辑表示的目标转化为一般化的实现型目标和维持型目标以便用于实际使用。Dastani等人提出操作化（Operationalization）方式，根据时序连接符不同，对时序逻辑表示的目标进行合理转化，使其转变为一个或多个逻辑对应的实现型目标和维持型目标。然而，该框架并没有提供一个一般性的意图调度算法，某些特殊形式的目标仍然需要用户自定义条件-动作对，虽然有一点的拓展性，但加重了用户的负担；此外，该框架也没有考虑到社会仿真场景下的norm，这导致其不适用于norm约束场景。

综上，目前对智能体在维持型目标的支持、norm约束下的决策以及对智能体目标种类的拓展方面的研究都取得了重要的进展。然而现有研究依然存在一些问题与空白。

现有的BDI模型不支持维持型目标，且目前对维持型目标研究也仅仅考虑单个目标的执行对维持型目标的影响，而没有考虑多目标并行执行的情况，因此无法有效解决引入维持型目标带来的问题。

现有的BDI慎思过程框架缺乏对norm的支持，使得智能体在决策时无法考虑到norm对其行为的约束作用，进而无法做出合理的决策；同时，现有的对BDI智能体意图调度的相关研究也并没有考虑到norm对智能体的影响，使得其无法用于解决norm约束下智能体意图调度问题。

现有的相关研究大多仅针对社会仿真场景下的某一方面的问题（例如仅针对norm或维持型目标），而没有考虑norm和维持型目标共存的情况，缺乏一般性。

\section{信念、计划以及动作的规范表示}
本章节主要介绍BDI智能体信念、动作以及计划这三个基本元素的规范表示，在后续的章节中根据问题场景不同将会介绍到BDI智能体其他内部状态的规范表示。
\paragraph{信念}
信念为智能体对环境及其自身的认知，一个信念以一个三元组表示：
$$<B,T,V>$$
其中$B$是一个标识符，$T$定义了$V$所表示的数据类型，而$V$则存储了该信念的实际数据。例如$<FuelLevel,Number,60>$这条信念表示智能体当前的电量为60。
\paragraph{动作}
动作是智能体可以执行的最小单元，其可以直接对环境造成影响。一个动作以一个三元组表示：
$$<A,C_{pre},C_{post}>$$
其中$A$为该动作的名称，$C_{pre}$和$C_{post}$分别对应该动作的前置条件和执行结果。每个条件（前置条件或执行结果）由四部分组成：条件的名称$C$、条件所对应的信念$B$、比较运算符$O$以及数值$V$：
$$<C,B,O,V>$$
例如，当智能体当前的油量大于20时，条件$<Fuel>20? FuelLevel,>,20>$会返回真。
\paragraph{计划}
每个计划P以$G:\emptyset \gets \alpha_1; \dots;\alpha_n$的形式表示，其中G为一个目标（实现型或维持型目标），$\emptyset$是一组命题公式，为该计划的前置条件，只有当$\emptyset$为真时，P才能被执行。最后，$\alpha_1; \dots;\alpha_n$为计划中的一些列执行步骤，计划的执行对于依次执行这些执行步骤。每个执行步骤或为可以直接改变环境的动作或为子目标，而子目标可其对应的子计划实现。

\section{目标计划树}
% ref
Thangarajah等人提出目标计划树（Goal-Plan Tree）\cite{DBLP:journals/jar/ThangarajahP11,DBLP:conf/ijcai/ThangarajahPW03,DBLP:conf/ijcai/ThangarajahPW03,DBLP:conf/ecai/ThangarajahWPF02}的数据结构来表示BDI智能体目标及计划之间的关系。在此基础上，Yao等人提出了对目标计划树的拓展\cite{DBLP:conf/atal/YaoSL16}，增加了对动作的表示，并定义了包含多种可能执行结果的易错动作和有执行时间的持续型动作。目标的定义也被拓展为含有截止时间和期望完成时间。考虑到目标计划树的表达能力，本文将使用Yao等人提出的拓展目标计划树来表示智能体的意图。

% GPT
目标计划树的根节点为顶层目标节点，即智能体想要达到的最终目的；其孩子节点为一个或多个计划节点。每个计划节点的孩子节点为一系列执行步骤（动作或子目标）；而子目标又有其对应的计划节点，如此便构成了树形结构，其展现了所有实现顶层目标的途径。此外，为了实现某个目标，智能体只需要选择一个计划执行即可，因此计划节点也被视为“OR”节点；而为了完整执行一个计划，智能体需要依次成功执行其对应的所有执行步骤，因此计划节点的孩子节点被视为“AND”节点。

图\ref{fig:gpt}展示了火星探测器领域下的一个简单目标计划树模型，其中环境为$h*w$的网格。火星探测器智能体可以通过执行动作在不同的方格中移动。其顶层目标$G0$表示将智能体移动到$(a,b)$位置。有五个不同的计划可用于实现$G0$，不同的计划可应用于不同的环境状态下。如果智能体当前位置位于$(x,y)$且$y < b$，则计划$P0$可用于实现$G0$。P0由两个步骤组成，一个是可直接执行的动作$A0$，执行$A0$可以使智能体向上移动一个单位，第二个是子目标$G0$，$G0$为递归目标（即该子目标实际与顶层目标相同）。$A0$的前置条件为智能体不在网格的上边界（在上边界执行该动作会使得智能体超出边界范围），而其后置条件则为将智能体位置至于$(x,y+1)$。另外，该目标计划树示例将用于后续章节的实验部分。
\begin{figure*}[htb]
\centering
\includegraphics[width=0.9\textwidth]{./figs/MarsRover_GPT}
\bicaption{目标计划树例子}{Example goal-plan tree.}
\label{fig:gpt}
\end{figure*}

% Benefits of GPT representation
目标计划树直观地展现了智能体目标、计划和动作之间的关系。除此以外，目标计划树还可用于记录实现目标或执行计划和动作所需的前置条件（Precondition）以及执行的结果，后置条件（Postcondition）。计划的前置条件指的是执行计划前需要满足的条件，如果前置条件不满足，则计划无法执行（动作的前置条件同理）。后置条件指的是执行动作之后的结果，对智能体所处环境造成的影响。最后，目标计划树还可以用于刻画智能体在某个环境下实现特定目标时的健壮性，Thangarajah等人基于目标计划树提出Coverage和Overlap的概念\cite{DBLP:conf/aamas/ThangarajahSP12}以刻画智能体程序的健壮性。其中Coverage表示一个意图当前的目标或子目标至少有一个计划可执行情况下可能的环境状态占比；而Overlap则表示一个目标有多个（大于一个）可执行计划的情况下可能的环境状态占比。

\section{意图进展问题}
上一小节介绍了目标计划树的概念，本章节基于目标计划树，对BDI智能体意图进展问题进行规范定义。

BDI智能体通常有多个计划用以实现某一个目标，不同的计划根据其前置条件不同适用于不同的环境场景，前置条件满足的计划被称为可应用计划（Applicable Plan）。
% plan selection
当同时有多个可应用计划是，智能体需要考虑选择哪一个计划用以实现相应目标，该问题被称为计划选择问题（Plan Selection Problem）。计划选择会影响到并发执行的其他目标，因为不同的计划有着不同的执行结果，这些执行结果可能会影响其他目标下计划或动作的前置条件。这种影响可能是负面的，例如一个计划执行之后使得其他目标下的某些计划的前置条件失效，也有可能是正面的，例如一个目标下的某些计划的前置条件在初始时是不满足的，当执行其他目标下的计划之后使得其前置条件满足。

% intention selection
另一方面，当智能体有多个意图时，下一步该执行哪个意图的问题被称为意图选择问题（Intention Selction Problem）。意图选择问题同样也会影响目标的实现，不合理的意图选择可能会造成各个意图中的前置条件相互破坏，导致无法实现任何一个目标（单个目标本身可以顺利实现）,而合理的意图选择则可以使得多个意图产生协同效应，相互促进执行。

% IPP
意图进展问题（Intention Progression Problem）为上述两问题的结合，即同时考虑计划选择问题和意图选择问题。意图进展问题是本文所研究的问题的基础。为了规范并一般化地表示意图进展问题，本文基于目标计划树对意图进展问题进行定义。一个意图的进展实际对应于一个目标计划树中一条从根节点到一个叶子结点的路径。目标计划树中的任何一条从根节点至叶子节点的路径都对应于实现其顶层目标的一种方式。每一条路径都包含一系列的计划、动作和子目标，如果他们都被成功执行，即可实现顶层目标。如此，智能体的执行过程可看作多个目标计划树执行路径的相互交错，即对应于意图进展的并发执行。

%
在不同场景下对智能体性能的评估标准可能不同，例如，有时智能体完成目标的数量越多越好，而有时公平性更重要，多个目标的完成时间越接近越好。为了在社会仿真模拟场景下提供一个一般化的意图调度方法，本文假设对某一场景下的智能体有一价值函数$f_{\mu}$，$f_{\mu}$的输入参数为智能体状态信息，并返回一个评估值，代表智能体当前的性能表现。$f_{\mu}$函数可由用户自定义，以应用于不同的问题场景。

% definition
基于上述内容，意图进展问题可被理解为：如何选择多个目标计划树的交错执行路径，使得最终基于$f_{\mu}$获得最大的价值。具体的，给定一组表示智能体意图的目标计划树集合$\{t_1, \dots, t_n\}$，当前环境信息$Env$，智能体的初始状态$S_0$，需要实现的目标集合$G_0$以及当前问题中的价值函数$f_{\mu}$，智能体在每一个运行周期中求解并执行目标选择，计划选择以及意图选择，直到某个最终状态$S$停止运行，要求最终状态$S$使得有效函数$f_{\mu}$的值最大，即智能体从初始状态$s_0$开始进行目标计划树的交错执行，不存在某个最终状态$S^{\prime}$，使得$f_{\mu}(S^{\prime})$的值大于$f_{\mu}(S)$。
