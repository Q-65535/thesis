\chapter{绪论}
\section{智能体简介}
%智能体
随着近年来人工智能领域的高速发展，计算系统的智能性受到越来越多人的重视。智能体(Agent)是嵌于特定环境中能够自主行动 以实现给定设计目标的个体\cite{DBLP:books/daglib/0023784}。多智能体系统 Multi-Agent System, MAS）\cite{DBLP:books/daglib/0023784}通过智能体间的交互实现分布式智能，能够用于解决单个智能体难以或者无法解决的问题，是当前人工智能领域的 研究热点之一。多智能体相关的研究成果已经广泛应用于工业制造\cite{biernatzki2004agent} 、城市交通等重要领域\cite{DBLP:journals/tits/ChenC10}。

%BDI智能体
在当前针对智能体与多智能体系统的研究中出现了多种用于实现自主智能体的智能体体系结构，包括慎思型结构、反应型结构以 及混合型结构\cite{DBLP:conf/atal/2000, DBLP:journals/ker/WooldridgeJ95}等。
% BDI
其中，基于心理学实践推理(Practical Reasoning)\cite{bratman1987intention}的 Belief-Desire-Intention-based (BDI)模型\cite{DBLP:conf/atal/GeorgeffPPTW98} 是当前学术界应用和研究最多的智能体体系结构之一。
%
采用 BDI模型实现的智能体（BDI 智能体）使用信念（Belief）、愿望 （Desire）以及意图（Intention）等概念来表示自身的心智状态，并通过实践推理来决定下一步应该采取的行动。
%
智能体的信念指的是智能体对其所处环境以及自身的认知，即智能体相信什么；愿望（又被称为目标）指的是智能体想要达到的世界状态的集合。智能 体通过执行计划（Plan）来实现其给定设计目标。计划由 顺序执行的一系列步骤组成。每个步骤是能够改变当前世界状 态的基本动作，或者是需要由智能体实现的子目标。
%
当智能体选择 某一特定计划来实现其给定目标时，该计划未执行的部分被称为实 现该目标的意图，即智能体承诺为了实现目标而执行的步骤。

到目前为止已有许多基于BDI智能体架构的编程语言和平台被提出并广泛应用。最早基于BDI架构的智能体系统是Procedure Reasoning Systems(PRS)，其被开发用于航空领域\cite{DBLP:conf/aaai/GeorgeffL87}以及网络交通领域\cite{DBLP:conf/aaaiss/Wobcke07}。AgentSpeak(L)\cite{DBLP:conf/maamaw/Rao96},一种抽象的BDI智能体编程语言，主要目的是帮助人们理解BDI架构在实际中的应用（例如PRS）与BDI架构理论模型直接的关系。Jason\cite{bordini2007programming}是一门基于AgentSpeak(L)开发的实用性语言，其语言的底层实现基于JAVA，有着很好的跨平台特性。除此以外，还有许多其他基于BDI架构的语言和平台，例如2APL\cite{DBLP:journals/aamas/Dastani08}，3APL\cite{DBLP:books/sp/map2005/DastaniRM05}以及JACK\cite{DBLP:books/sp/map2005/Winikoff05}。

目标与意图是BDI智能体的核心。在 BDI 模型中，一个目标通 常对应有多个不同的实施计划以应对不同的环境状态。智能体需要 选择执行其中的一个计划来实现其自身目标。该问题被称为 BDI 智 能体的计划选择（Plan Selection）问题\cite{yao2017robust}。在许多实际问题中， BDI 智能体都会被同时赋予多个目标进而产生多个并行意图。此 时，BDI 智能体需要决定下一步应该执行哪一个意图。该问题被称 为 BDI 智能体的意图选择(Intention Selection)问题\cite{yao2017robust}。在\cite{DBLP:conf/emas/Castle-GreenDL20}中，智能体的计划选择与意图选择被称为智能体慎思过程中的意图 进展问题（Intention Progression Problem， IPP），即 BDI 智能 体在每一个慎思周期中都需要选择其下一步需要执行的意图，如果 被选择的意图的下一步是子目标，那么BDI智能体还需要选择实现目标所需要的计划。


IPP是BDI智能体研究中的一个关键问题，任意地进行意图进 展可能会导致不同意图间的冲突，例如，智能体$\alpha$有两个目标：前往地点A和购买电池；前往地点A可由两个计划实现：行走前往和打车前往。若$\alpha$先选择打车前往 A，则可能由于打车的资金花费而导致后续没钱购买电池，而选择行走前往则不会造成该冲突。

\section{国内外研究现状}
本节对现有相关研究进行文献述评，分别考虑了对经典意图进展问题的相关研究，对维持型目标的相关研究以及norm约束下决策问题的相关研究。
% 考虑分三点：1.传统意图进展问题，2.norm约束下的决策，3.维持型目标下的决策
\subsection{经典意图进展问题研究现状}
% SI
在对意图进展问题的研究中，Thangarajah等人\cite{DBLP:journals/jar/ThangarajahP11,DBLP:conf/ijcai/ThangarajahPW03}提出了基于SI （Summary Information）的意图调度算法，其通过自底向上总结信息的方式确定实现目标的必要条件、可能条件、必要结果以及可能结果来推理出意图之间是否存在冲突以及是否存在协同效应。SI算法在智能体程序编译期进行信息总结计算，并在运行期间随着智能体意图的进展动态更新总结信息。此外，SI支持实时判断接收新目标的机制，允许智能体在运行时根据总结信息判断新的目标是否一定会、可能会或一定不会与当前目标相冲突，然后决定是否接收新的目标。然而，该方法假设现有目标的优先级总是大于新目标，若发生冲突则不接收新目标，而不会考虑抛弃现有的冲突目标。

% C
Waters等人\cite{DBLP:conf/atal/WatersPS14,DBLP:conf/aamas/ThangarajahSP12}提出了CB（Coverage Based）意图调度算法，并在后续研究中对其进行了改进\cite{DBLP:journals/aamas/WatersPS15}。CB考虑在编译期对实 现目标计划的健壮性进行数值化刻画，提出在不同环境下智能体实 现某个目标的可执行性（Coverage），以及在同一环境下智能体实现 某个目标的计划选择多样性（Overlap）。在意图进展过程中CB优先考虑实现脆弱的目标，以免因为环境自身的动态性导致目标因无可选计划执行而无法实现。相较于SI，CB重点考虑的是智能体在动态环境下的意图进展。

% MCTS
Yao等人提出了基于蒙特卡洛树搜索（Monte Carlo Tree Search，MCTS）的意图调度算法\cite{DBLP:conf/aaai/YaoLT16,DBLP:conf/atal/YaoL16,DBLP:conf/ecai/YaoLT14,DBLP:conf/atal/Yao21,DBLP:conf/ijcai/Yao20,DBLP:conf/ecai/YaoLT16}，其利用MCTS的特点，专注于对最有前景的行为进行搜索，并基于随机模拟构建并不断扩展搜索树，最终根据用户自定义的评判标准和搜索树记录信息得到决策结果。和SI以及CB不同，MCTS算法的执行发生在程序运行期，其总是基于当前状态对决策空间进行搜索，并得出当前决策结果。MCTS调度策略被应用于消 解并行意图间的冲突、开发并行意图交错执行的协同效应、对时间 限制的意图进行调度以及在动态和不确定环境下的意图调度。此外，根据对目前该研究领域的调研，得知基于MCTS的意图调度算法 是当前可验证的最好的意图调度策略。
% 传统意图调度方法的问题
然而，上述方法都没有考虑到社会模拟场景中维持型目标对智能体的影响，缺乏在意图调度是考虑维持型目标的能力。


\subsection{维持型目标研究现状}
虽然 BDI 智能体在多个领域得到广泛应用，但是在智能体目标的表示与推理机制方面，以往相关研究大多聚焦于实现型目 标，而没有考虑到维持型目标。维持型目标定义了一些情况下以维 持某个状态一段时间为目标，例如火星探测器需要在勘探过程中维 持电量在某一数值之上，以确保其能够在勘探之后顺利返回基地补 充电量。Dastani等人\cite{DBLP:conf/atal/DastaniRM06} 描述了维持型目标，刻画了智能体对维持某 一个状态需求的基本模型。然而，现有意图调度算法大多仅考虑了实现型目标，即到达某个世界状态，而没有考虑维持某个世界状态 的维持型目标。目前仅有的对维持型目标加入考虑的意图调度算法\cite{DBLP:conf/atal/DuffHT06,DBLP:journals/ci/DuffTH14,DBLP:conf/dalt/HindriksR07}并没有考虑到多意图并行交错执行对维持型目标的影响。

% maintanance goal
在对维持型目标的研究中，Duff等人提出一种基于SI的意图调度算法\cite{DBLP:conf/atal/DuffHT06}，其在决策过程中加入对维持型目标的考虑：若实现型目标将与维持型目标冲突，执行预防措施（例如补充资源）。 然而，和SI类似，该算法假设当前的目标优先级总是高于新的目标，并且如果新的目标为维持型目标且与当前目标相冲突，维持型目标将被直接抛弃。后来Duff等人在\cite{DBLP:journals/ci/DuffTH14}中拓展了维持型目标的规范语法及语义，但是意图调度策略并没有被改进。Hindriks等人\cite{DBLP:conf/dalt/HindriksR07}提出了一种处理维持型目标的决策机制，该机制直接避免执行破坏维持型目标的行动，保证维持型目标不会被破坏。然而该方法将会导致智能体可执行的行动变少，降低智能体的灵活性。Thangarajah等人\cite{DBLP:conf/ecai/ThangarajahHMY14}分析了维持型目标的进展过程，提供了一套合理的维持型目标进展模型，但是起没有考虑到意图进展问题。

目前在BDI智能体研究领域中，已有多种对智能体的功能性进行拓展的方法被提出以提升其适用性。鉴于大多数对智能体目标的研究 都着重关注实现型目标和维持型目标，Dastani等人\cite{DBLP:conf/atal/DastaniRW11}提出以时序逻辑来表示目标，以拓展智能体支持的目标种类。在他们提出的框架中，目标的表示不仅包含基本命题，还包含时序连接符号如$\diamond$（最终），$\square$（总是）以及$U$（直到）。在具体应用时，由于大多数智能体语言和平台都不支持时序逻辑，需要将时序逻辑表示的目标转化为一般化的实现型目标和维持型目标以便用于实际使用。Dastani等人提出操作化（Operationalization）方式，根据时序连接符不同，对时序逻辑表示的目标进行合理转化，使其转变为一个或多个逻辑对应的实现型目标和维持型目标。然而，该框架并没有提供一个一般性的意图调度算法，某些特殊形式的目标仍然需要用户自定义条件-动作对，虽然有一点的拓展性，但加重了用户的负担；此外，该框架也没有考虑到社会仿真场景下的norm，这导致其不适用于norm约束场景。
\subsection{Norm约束下的决策问题研究现状}
% 社会模拟场景
BDI智能体在社会模拟场景中也有广泛的应用\cite{DBLP:conf/ijcai/SinghSPJ11}，其类人的内部心智状态表示以及行为推理方式使其能够灵活地适应于多种社会模拟场景。BDI智能体程序自然假设环境是动态的，能够在不同情 况下实时选择合适的方式实现目标；另外，BDI 智能体有能力处理执行失败的情况，当某个计划执行失败时，其可以通过失败恢复机 制重新考虑如何实现当前目标。此思考与运行机制使得BDI智能体拥有高度适应性与灵活性，并且与人类操作方式非常类似，这使其有能够合理应用于社会模拟场景。

% Norm
在一些社会模拟场景中，环境是开放的，不同类型的智能体可以随意地选择离开或是加入，它们的行为能力与目标也不尽相同。这些特性使得开放多智能体系统的运行状况与结果难以预测与控制。当一些智能体实施恶意行为时，其会对其它正常运行的智 能体甚至整个系统造成严重损害。为了保证环境的可控性与稳定性，norm的概念被提出作为管控智能体行为的一种方式\cite{DBLP:journals/mags/SavarimuthuC11}，其对智能体的行为在一定条件下加以特定约束，用于确保智能体 在环境中的合理、合规地运作。norm 通常定义了系统中智能体‘道义’方面的行为准则。Norm 以义务（Obligation）来规范智能体在 某些情景下应该去做什么，例如在超市里，智能体应该付完钱之后再带着商品离开；norm以许可（Permission）来规范智能体在特定 场景下可以做什么，例如智能体在环境中移动是被许可的；norm也可以禁令（Prohibition）来规范智能体在特定场景下不能去做什么，例如在开放多智能体系统中攻击或伤害其它智能体是被禁止的。Norm对智能体的约束可能会对智能体实现目标的过程造成阻碍，或是限制智能体某些特定的实现目标的方式。盲目的、不考虑 norm的智能体运行机制势必会导致低效的执行效果。例如，当一个智能体正常前往某个目的地需要过马路时，它需要考虑交通规范norm对其的约束：在红灯时不能过马路（即使它有能力这么做，否则，将受到罚款。另一方面，若智能体需要尽快到达目的地时（例如救火），其可以考虑违反交通规范norm，优先实现更为重要的目标，若智能体没有考虑norm对其实现目标的影响的能力，则无法做出合理的决策。因此，一个高效的、能够对 norm进行合理考虑的智能体意图调度机制对构建适应性强、运行在norm作用环境下的智能体有十分重要的意义。
规范实践推理（Normative Practical Reasoning，NPR）指的是智能体在实践推理的过程中考虑norm对其影响。在其相关研究领域中，研究者们提出了多种考虑norm的智能体决策方法。

% norm决策
% BOID
对NPR的研究最早或许可以追溯到BOID架构\cite{DBLP:conf/agents/BroersenDHHT01}，BOID架构实质上是对BDI架构的一种延申，赋予其决策时考虑norm的能力。BOID在BDI的基础上引入了智能体性格类型的概念以处理norm与行动、目标之间的冲突。BOID智能体可以是社会型的、无视规范型的等，不同类型的BOID智能体对norm有不同的解释，在解决norm相关冲突时也有不同的策略。例如无视规范型的智能体会更倾向于考虑自身目标的实现，而不过多考虑norm对其约束，在norm相关冲突发生时忽略norm的存在。BOID智能体的决策过程是纯粹基于命题规则的。如果在其决策过程中某些规则发生冲突（可能由norm造成），智能体将基于其自身类型进行冲突消解以得到最早确定的决策结果。BOID架构的设计非常简洁，易于理解，但是这种架构并没有广泛的实际应用。很大 一部分原因在于其健壮性低下，仅依靠逻辑推理做出决策，决策过程的时间复杂度高，当规则量庞大时，极其耗时，且不易维护。

% NoA
NoA\cite{DBLP:conf/ijcai/KollingbaumN03}架构基于BDI架构并引入了norm的相关概念，例如义务，禁令以及许可。许可规范了智能体可以做什么（而不一定要去做）。在决策阶段，NoA架构规范了智能体该从计划库中选择哪个计划去执行以遵守norm的相关约束。NoA是完全以norm为主导的架构，它没有自身内部的目标，因此不需要考虑norm与目标的冲突。这使得NoA总是会做出遵守norm的行动。NoA能够处理norm的相关概念，其架构设计也较为简易,易于理解。然而--中仅使用文字对NoA的概括性描述，而缺乏对NoA架构抽象准确的形式化定义。没有内部状态，完全以norm为导向的设计也使得NoA架构适用场景受限，限制了BDI智能体的基本自主性。


基于norm的行为修改（Norm-based Behavior Modification，NBM）\cite{DBLP:conf/atal/MeneguzziL09}是对基础BDI架构的一种延伸，其解决的问题主要是智能体在运行时如何去适应新的norm，NBM架构的重点在于提供一种机制使得智能体有能力在运行时去约束一些计划的执行以适应其在运行时所检测到的新的norm。其优点在于其方法依然在BDI的框架下，对于新的norm，创建一个新的计划来处理，将那些norm禁止的计划进行限制，不让其在后续norm生效的情况下执行。该机制可以方便地部署到其它符合BDI架构的智能体系统中。虽然本文实现了在运行时应对新检测的norm的机制，但是没有对norm对于智能体的影响进行精确地刻画，简单认为norm禁止的计划就不能去执行，使得智能体灵活性降低。其研究重点关注的问题也仅限于计划选择，目标选择并不在其考虑范围之内。

% N-2APL
N-2APL\cite{DBLP:conf/aamas/AlechinaDL12}是基于2APL\cite{DBLP:journals/aamas/Dastani08}的智能体编程语言。 N-2APL在2APL的基础上拓展了对norm的表示与处理机制，支持表示义务、禁令等概念，支持在决策过程中处理norm带来的冲突。N-2APL的目标选择与计划选择机制是基于实现目标截止期限以及计划优先级制定的。N-2APL假设计划优先级由编程人员根据norm的影响提前定义。其决策过程主要为： 1.放弃某些低优先级的目标以确保高优先级的目标能够在截至期限内完成；2.选取一个高优先级且能够在截止期限实现的目标；3.根据提前定义好的优先级顺序选择计划执行。
在N-2APL中，有限制条件的义务并没有被考虑，使其应用场景受限。N-2APL通过引入原子计划与非原子计划的概念，以降低可能的执行序列的数量来控制决策算法的时间复杂度并消解计划间并行执行的冲突：原子计划执行时不能被打断，而非原子计划执行过程中可以被其他计划打断。这种设计需要编程人员手动对计划进行分类，不利于大规模实践应用；由于可能执行序列的减少，这种设计也会一定程度上降低了BDI智能体的灵活性。

% N-Jason
N-Jason\cite{DBLP:conf/dalt/LeePLDA14}是一个基于Jason\cite{bordini2007programming}智能体编程语言，支持运行时处理未知norm智能体编程语言。相较于以往的norm处理机制——假设norm在运行前就已经确定，在运行时也不会改变， N-Jason支持运行时对未知norm的识别。如果N-Jason识别到义务norm，则将其转化为一个新的目标待后续阶段实现；如果识别到禁令norm，则将其加入禁令norm集合，供后续进行冲突消解阶段应用。与N-2APL类似，N-Jason 也支持基于截止期限与norm的影响对目标、 计划划分优先级以消解冲突并选择合适的计划执行。然而，不像N-2APL，N-Jason没有考虑多目标之间的选择问题，仅考虑了如何选择计划。作者也提到基于N-Jason产生的的智能体行为可能在编程人员看起来是不可解释、不可预测的，然而却没有提供确切的理论或实践依据。

% v-BDI
v-BDI\cite{DBLP:journals/eaai/MeneguzziROVL15}对BDI架构进行了拓展，在决策时加入了对norm的考虑，其允许智能体在norm冲突发生时，根据自身认识自主地选择是否遵守norm，v-BDI的关键技术在于在计划选择时先对各计划的行动进行模拟运行，然后根据模拟结果对比各个计划的价值，最终选择出价值最大的计划执行。v-BDI依据计划价值来进行计划选择， 当执行计划的收益大于违反norm受到的惩罚时，智能体可以选择去做出违反norm的行为以获得更大的整体收益。这种机制保留了BDI智能体的灵活性与自主性，同时计划选择的标准由模拟结果决定， 不需要编程人员明确给的。编程人员仅需要定义模拟的价值函数即可。v-BDI中对计划价值的计算仅考虑其中的行动，而不考虑子目标，这使得价值计算过程的时间复杂度较低，易于实际的应用。然而v-BDI仅考虑到了计划选择问题，而并没有提及多目标情况下的目标选择问题。仅考虑计划中行动的模拟机制虽然高效，但是必然会因为没有考虑到子目标而导致价值计算不准确。

此外，在非BDI架构领域，也有一些对norm考虑的决策机制被提出。相较于BDI架构，非BDI架构的智能体没有事先由编程人员定义好的计划库，而仅有由一个个独立的行动组成的行动集合。因此，设计非BDI架构智能体norm决策机制最大的挑战在于如何在norm规范下根据一定的规则，基于行动集合生成某些执行序列以供智能体执行，达到设计者的设计目标。

%
Shams等人\cite{DBLP:conf/atal/ShamsVPV15}提出的方法使用到了提前设定好的计划，但是其和BDI智能体的差异在于Shams等人提出的决策过程允许智能体根据norm的影响动态调整执行计划。其在运行时选择执行序列并将其与当前计划相比较，若新执行序列组成的计划的收益大于当前计划，则将新生成计划作为当前计划执行。Norm对智能体行为的约束体现在若智能体做出违反norm的行为将受到一定数值的惩罚。各行动受惩罚值的大小决定了智能体会如何执行计划：智能体总是会执行整体收益（实现目标的收益+违反norm的惩罚）最大的计划。该方法能够通过搜索得到最优的计 划，但是其时间复杂度较高，若可选计划数量较多，则实际应用较为困难；同时其也没有考虑到多个并行执行的计划之间的相互冲突与影响。

% RL
Li等人\cite{DBLP:conf/atal/LiMFL15}提出了使用强化学习方法对变化的或位置的norm进行学习，并根据学习结果对智能体自身的行为进行动态调整，以更好地在norm存在的环境中实现目标。Li等人\cite{DBLP:conf/atal/LiMFL15}分别对两种强化学习方法 Q-Learning 和 SARSA\cite{Sutton2005ReinforcementLA}进行了测试，验证了其有效性并对比分析了 两种方法的特点与适用场景：Q-Learning更加激进，对norm的学习的速度很快，但是不太稳定；SARSA比较保守，对 norm的学习速度慢，但是稳定性高。与—类似，\cite{DBLP:conf/atal/LiMFL15}没有考虑多目标之间的冲突问题，并且当问题规模较大时，这两种基础强化学习方法需要消耗大量计算时间，不利于实际应用。

% argumentation based
Shams等人\cite{DBLP:conf/ijcai/ShamsVOP16,DBLP:journals/taas/ShamsVOP20,DBLP:conf/atal/ShamsVPV15}提出的方法在不同场景设定下对 norm 的考虑主要体 现在可解释性，其实用 Argumentation Dialog 的方法试图对智能体 在 norm 约束下的行为进行解释，以提高人类对智能体程序的信任 度。同时，其考虑到了 norm 与智能体目标之间的冲突问题，提供了 一套消解冲突的机制。然而，与\cite{DBLP:conf/atal/LiMFL15}类似，在各个场景下该机制的 时间复杂度较高，并且没有考虑多个目标之间的冲突。


综上，目前对智能体在维持型目标的支持、norm约束下的决策以及对智能体目标种类的拓展方面的研究都取得了重要的进展。然而现有研究依然存在一些问题与空白。

现有的BDI模型不支持维持型目标，且目前对维持型目标研究也仅仅考虑单个目标的执行对维持型目标的影响，而没有考虑多目标并行执行的情况，因此无法有效解决引入维持型目标带来的问题。

现有的BDI慎思过程框架缺乏对norm的支持，使得智能体在决策时无法考虑到norm对其行为的约束作用，进而无法做出合理的决策；同时，现有的对BDI智能体意图调度的相关研究也并没有考虑到norm对智能体的影响，使得其无法用于解决norm约束下智能体意图调度问题。

现有的相关研究大多仅针对社会仿真场景下的某一方面的问题（例如仅针对norm或维持型目标），而没有考虑norm和维持型目标共存的情况，缺乏一般性。
在真实社会场景中，复杂多变的环境往往需要智能体有更强的功能性与灵活性，例如要求智能体同时处理各种类型的目标以及norm。然而现有研究大多都仅针对解决某一特定的需求或增强某一特定的功能，在维持型目标的研究中没有考虑到norm的影响，另一方面在norm的相关研究中没有考虑到其他更加复杂种类的目标。这种现状一定程度上限制了BDI智能体的一般性与通用性。

\section{立题依据}
% 问题
根据上述对研究背景和意义的介绍与分析，得知当前在BDI智能体决策研究领域有如下问题：
\begin{enumerate}
  \item 现有关于智能体决策的研究大多针对实现型目标，而极少考虑到维持型目标，特别是实现型和维持型目标共存的情况。另一方面，在针对维持型目标的研究中没有考虑到多意图并发执行的问题。
  \item 现有的关于NPR研究中没有考虑到多意图并发执行的情况，意图之间的交错执行与相互影响被忽略。这使得智能体无法在norm约束下高效实现多个目标。另一方面，大多现有的关于多意图调度算法中没有考虑到norm对智能体的影响。
  \item 维持型目标与norm共存的情况再模拟社会场景中并不少见，然而现有研究大多仅针对解决一个方面的问题（或维持型目标或norm），缺乏一般性。
\end{enumerate}

%
为了解决上述问题，本文充分研究了社会仿真模拟场景下智能体的行为决策相关技术，并基于随机采样模拟和时序逻辑提出一系列BDI智能体意图调度方法：
\begin{itemize}
  \item
针对问题1：本文将研究并实现对维持型目标全面支持的智能体意图调度方法，该方法在SA的基础上进行拓展，在决策时加入对维持型目标的考虑，使得智能体能够在意图调度过程中考虑到维持型目标，拓展智能体的适用领域。根据智能体对环境的确定性不同，该方法考虑了主动的维持型目标（在维持型目标被破坏之前尝试修复操作）以及被动的维持型目标（在维持型目标被破坏之后尝试修复操作），以适应不同的应用场景。
  \item
针对问题2：为了实现 norm 约束下的意图调度算法，本文将对智能体的norm约束问题模型，多目标交错执行触发 norm 问题以及 norm 对智能体决策的影响问题展开研究，提出norm约束下的意图决策方法，该方法在SA算法的基础上进行拓展，对其运行时的多个阶段进行拓展，加入对norm的考虑，实现智能体高效地在 norm 约束环境中并行实现多目标能力，提高智能体的适用性。
  \item
针对问题3：本文在对前两个问题的研究基础之上，提出基于时序逻辑（Linear Temporal Logic，LTL）的意图调度算法，使得智能体可以同时处理norm与维持型目标。此外，由于时序逻辑的一般性，使得该方法有较强的拓展性，其可适用范围不限于norm与维持型目标，而可被应用于用户自定义类型的目标（例如条件限制的维持型目标或实现型目标）或是其他场景约束。
\end{itemize}
%
\section{论文主要内容及章节安排}
本文的章节安排如下。第二章首先介绍智能体的基本背景概念，再引出一种热门的智能体架构-BDI智能体架构；然后对现有相关研究内容进行文献述评，分析不同研究所提出方法的特点以及局限性。

第三章中首先对BDI智能体的相关概念如信念、意图等进行规范定义，并介绍使用目标计划树的数据结构表示智能体意图的方法；然后，基于目标计划树模型对研究问题，即意图进展问题进行规范形式化定义。

第四章介绍本研究所基于的核心算法蒙特卡洛树搜索（Monte-Carlo Tree Search，MCTS）。并对其算法进行细致解释。最后，介绍MCTS在目标计划树模型下的具体应用：\SA 算法，为后续章节中的算法解释提供基础。

第五章提出一种基于\SA 的意图调度算法\SAM ，\SAM 支持同时对实现型和维持型目标的调度。其中维持型目标考虑到了在第\ref{background}中提到的被动维持型目标和主动维持型目标。另外，基于模拟火星探测器场景，对\SAM 的性能在动态和静态环境下进行了实验分析，并与Duff等人提出的PMG\cite{DBLP:conf/atal/DuffHT06}算法进行了比较，实验结果表明\SAM 的表现相对PMG有显著优势。

第六章提出一种基于\SA 的意图调度算法\SAN ，\SAN 支持在norm约束下对智能体意图进行调度。和对\SAM 的性能评估方式类似，在实验部分，对\SAN 的性能在动态和静态环境下进行了实验分析，并与Meneguzzi等人提出的v-BDI\cite{DBLP:journals/eaai/MeneguzziROVL15}算法进行了比较，实验结果表明SAN的性能相较于v-BDI有显著优势。

第七章提出一种以LTL对象为输入的意图调度算法\SAT，支持对各种以LTL形式表示的目标和norm进行意图调度。该章实验部分对\SAT 的性能表现在不同地形下进行了分析，并与Duff等人提出的PMG\cite{DBLP:conf/atal/DuffHT06}算法和Meneguzzi等人提出的v-BDI算法进行了比较，实验结果表明\SAT 的性能表现相对PMG和v-BDI有显著优势。

第八章对本文研究内容进行总结并对现有研究的不足之处进行分析，并在最后对未来可能的研究方向和研究热点进行展望。
