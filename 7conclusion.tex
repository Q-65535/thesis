\chapter{结论与未来展望}
\section{结论}
智能体系统在现今计算机相关领域有着广泛应用。智能体的自主性与灵活性等特点使得其很适合运行在复杂多变的社会仿真场景。在这种场景下，智能体可能需要同时实现多个目标。一个理性的智能体应该能够有效解决多意图并发执行下的意图调度问题。另外，在社会仿真场景中智能体的可用资源可能受限，智能体需要在一段时间内维持某一状态（例如保持资源充足）；同时还可能受到社会规范norm的限制，对智能体的行为造成影响。然而，现有研究对维持型目标存在下的意图进展问题以及norm约束下的意图进展问题都有一定的缺陷。
\paragraph{研究价值总结}
本文考虑了三种不同场景下意图进展问题：1.实现型目标和维持型目标共存下的意图进展问题；2.norm约束下的意图进展问题；3.以LTL表示的各种类型的目标与norm共存下的意图进展问题。相对应的，本文第\ref{mg}章提出\SAM 算法，使得智能体支持对维持型目标的调度；第\ref{norm}章提出\SAN 算法，在norm约束下对智能体意图进行调度；第\ref{ltl}章提出\SAT 算法，支持对LTL表示下的目标和norm进行意图调度。\SAM 、\SAN 和\SAT 均基于SA算法，而SA算法的核心在于使用随机模拟对不同的意图进展方式进行评估，并最终选择最优的步骤执行。用户可对该算法的模拟评估函数以及选择函数进行自定义，以适用于不同的问题场景。\SAM 、\SAN 和\SAT 算法的性能均在火星探测器模拟场景下进行了实验分析。为了对上述算法进行细致分析，在\SAM 和\SAN 的相关实验中，考虑到了静态的环境（全部目标在初始时一次分配）以及动态的环境（目标随着时间推进逐个分配）两种不同实验场景；对于\SAT 的实验，考虑到了智能体的低电量设定与高电量设定两种不同实验场景。最后，实验结果表明本文提出的算法性能相较于其他现有算法有显著的性能优势。
\paragraph{不足之处总结}
虽然本文提出的算法比现有其他方法表现更好，但其仍然有一定的不足之处：\SAM 虽然支持维持型目标，但是没有考虑到环境的不确定性，而在不确定环境下\SAM 对维持型目标的预测可能不准确；\SAN 仅支持对禁令型norm的处理，义务型和许可型的norm却没有考虑，另外，norm的不确定性也没有考虑，在某些环境中，智能体需要自行学习并推断norm的形式；\SAT 虽然支持各类TLT表示的目标和norm，但由于其在MCTS的选择和模拟阶段每一步骤都要额外考虑状态机的转移，这需要更多的计算时间，导致计算开销较大，可能不适用于需要即时反应的场景。
\section{展望}
由于\SAM 没有考虑到环境的不确定性，一个可能的研究展望是考虑如何在不确定性环境下对智能体的实现型目标和维持型目标进行调度。在norm约束场景中，未来可以探究智能体应该如何处理义务型和许可型norm，并研发相关学习算法使得智能体可以从观测或与环境交互中自行学习norm的相关信息。而对于LTL公式的相关调度算法，未来可以考虑如何降低计算开销，一种或许可行的方案是在MCTS扩展阶段进行减枝，抛弃掉不可能执行的动作以收缩搜索范围并节省内存，并在模拟阶段使用非完全随机的模拟执行策略，以此降低计算时间并提高价值评估的可信度。最后，可以考虑将\SAM 、\SAN 和\SAT 应用在不同智能体之间的交互中，以提高多智能体的执行效率。
